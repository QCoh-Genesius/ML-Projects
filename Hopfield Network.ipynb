{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSlFhbEubM3p"
   },
   "source": [
    "## Training a Hopfield network on MNIST\n",
    "\n",
    "**Objective:** Implement and train a Hopfield network. The goal is to reconstruct the perturbed/masked patterns.\n",
    "\n",
    "**Dataset:** MNIST\n",
    "\n",
    "**Tasks:**\n",
    "1. Data preparation: Binarize the MNIST images to have pixel values -1 or 1 (i.e., map pixels with value less than 127 to -1, and pixels with value greater or equal to 127 to 1). \n",
    "2. Choose a subset of 500 binarized MNIST images as above, and resize to 7x7 (by downsampling).\n",
    "3. Choose 6 of these patterns and design a Hopfield Network to memorize these patterns while discouraging the memorization of other patterns.\n",
    "4. Randomly pick one of the memorized patterns and flip 8 randomly-selected pixels of the corresponding image (i.e., changing $1 \\rightarrow -1$ and $-1 \\rightarrow 1$). Use the trained Hopfield Network to recover the original stored pattern.\n",
    "5. Repeat this process 500 times and report the success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uCdv4HM1elnU"
   },
   "outputs": [],
   "source": [
    "\"\"\" Data Preparation \"\"\"\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data and apply basic augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)),  # slight rotation and translation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "92ox4NiaevX9"
   },
   "outputs": [],
   "source": [
    "\"\"\" Hopfield network Architecture \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Parameter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Please implement the Hopfield network with pytorch\n",
    "class HopfieldNet(nn.Module):\n",
    "  # You can add other necessary parameters to be passed in the _init_ function.\n",
    "  # You can add other functions in the HopfieldNet class if necessary.\n",
    "    def __init__(self, pattern_size):\n",
    "        self.weights = np.zeros((pattern_size ** 2, pattern_size ** 2))\n",
    "\n",
    "    def train(self, patterns):\n",
    "        num_patterns = len(patterns)\n",
    "        # pattern_size = patterns.shape[1]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            pattern_flat = pattern.flatten()\n",
    "            self.weights += np.outer(pattern_flat, pattern_flat)\n",
    "        \n",
    "        self.weights /= num_patterns\n",
    "        np.fill_diagonal(self.weights, 0)\n",
    "\n",
    "    def energy(self, pattern):\n",
    "        return -0.5 * np.dot(pattern.flatten(), np.dot(self.weights, pattern.flatten()))\n",
    "\n",
    "    def update(self, pattern, max_iter=100):\n",
    "        pattern_flat = pattern.flatten()\n",
    "        for _ in range(max_iter):\n",
    "            new_pattern = np.sign(np.dot(self.weights, pattern_flat))\n",
    "            if np.array_equal(new_pattern, pattern_flat):\n",
    "                break\n",
    "            pattern_flat = new_pattern\n",
    "        return pattern_flat.reshape(pattern.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZmPw4o2UmP79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.174\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training\"\"\"\n",
    "# Implement a training process that is suitable for your HopfieldNet class\n",
    "# It is recommended to use \"import time, t0 = time.time(), print(f\"{i_epoch} ({time.time() - t0}s): {loss}\")\" to render the training time\n",
    "\n",
    "subset_indices = np.random.choice(len(train_dataset), 500, replace=False)\n",
    "subset_loader = DataLoader(train_dataset, batch_size=500, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices))\n",
    "\n",
    "# Get the sampled images\n",
    "subset_images, _ = next(iter(subset_loader))\n",
    "\n",
    "# Resize to 7x7\n",
    "subset_images_resized = F.interpolate(subset_images.view(-1, 1, 28, 28), size=7)  # Resize to 7x7\n",
    "subset_images_resized = subset_images_resized.view(-1, 7*7)\n",
    "\n",
    "# Normalize data between -1 and 1\n",
    "subset_images_resized = torch.where(subset_images_resized < 0.5, torch.tensor(-1.0), torch.tensor(1.0))\n",
    "\n",
    "train_patterns = subset_images_resized[:6]\n",
    "\n",
    "# Train the Hopfield Network\n",
    "hopfield_net = HopfieldNet(pattern_size=7)\n",
    "hopfield_net.train(train_patterns)\n",
    "\n",
    "# Step 4: Perturbation and Recovery\n",
    "def perturb_image(image, num_flips=8):\n",
    "    # Convert PyTorch tensor to NumPy array\n",
    "    image_np = image.numpy()\n",
    "\n",
    "    # Perform perturbation\n",
    "    perturbed_image = image_np.copy()\n",
    "    indices = np.random.choice(np.arange(image_np.size), num_flips, replace=False)\n",
    "    perturbed_image.flat[indices] *= -1\n",
    "\n",
    "    # Convert NumPy array back to PyTorch tensor\n",
    "    perturbed_image_tensor = torch.from_numpy(perturbed_image)\n",
    "    return perturbed_image_tensor\n",
    "\n",
    "def test_recovery(original_image, perturbed_image, hopfield_net):\n",
    "    recovered_image = hopfield_net.update(perturbed_image)\n",
    "    return np.array_equal(original_image, recovered_image)\n",
    "\n",
    "# Step 5: Success Rate Calculation\n",
    "success_count = 0\n",
    "num_trials = 500\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    index = np.random.randint(len(train_patterns))\n",
    "    original_image = train_patterns[index]\n",
    "    # print(original_image)\n",
    "    perturbed_image = perturb_image(original_image)\n",
    "    # print(perturbed_image)\n",
    "    if test_recovery(original_image, perturbed_image, hopfield_net):\n",
    "        success_count += 1\n",
    "\n",
    "success_rate = success_count / num_trials\n",
    "print(\"Success rate:\", success_rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
